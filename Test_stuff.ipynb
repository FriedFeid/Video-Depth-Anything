{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_video_length = 447\n",
    "frame_list = np.ones((447, 1242, 3))\n",
    "\n",
    "\n",
    "inference_length = 5 # Complete length of processed Context\n",
    "keyframe_list = [12, 5] # Distance to the last frame of the processed context. Exception 0 means always frame 0  START WITH HIGHEST DISTANCE, WE ALWAYS USE KEYFRAME 0\n",
    "align_on_each_new_frame = True\n",
    "max_keyframe = max(keyframe_list)\n",
    "\n",
    "assert inference_length > len(keyframe_list) + 2 # Because 0 is always a keyframe; 1 new Frame must be predicted \n",
    "\n",
    "# In Loop\n",
    "cur_frame = np.ones((1, 1, 3, 280, 924))\n",
    "layer_1, layer_2, layer_3, layer_4 = torch.ones((1, 48, 80, 264)), torch.ones((1, 96, 40, 132)), torch.ones((1, 192, 20, 66)), torch.ones((1, 384, 10, 33))\n",
    "\n",
    "motion_features = (np.ones((inference_length, 48, 80, 264)), np.ones((inference_length, 96, 40, 132)), np.ones((inference_length, 192, 20, 66)), \n",
    "                   np.ones((inference_length, 384, 10, 33)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 2, 5], [0, 1, 2, 6], [0, 1, 2, 7], [0, 1, 2, 8], [0, 1, 2, 9], [0, 1, 3, 10], [0, 1, 4, 11], [0, 1, 5, 12], [0, 1, 6, 13], [0, 1, 7, 14], [0, 1, 8, 15]]\n"
     ]
    }
   ],
   "source": [
    "# prepare tensor for saving: \n",
    "features_l1 = torch.zeros((inference_length+max_keyframe, *layer_1.size()))\n",
    "features_l2 = torch.zeros((inference_length+max_keyframe, *layer_2.size()))\n",
    "features_l3 = torch.zeros((inference_length+max_keyframe, *layer_3.size()))\n",
    "features_l4 = torch.zeros((inference_length+max_keyframe, *layer_4.size()))\n",
    "\n",
    "# We need the following lists: \n",
    "# Motion_feature_idx: Which motion features to keep -- Should be always the same \n",
    "\n",
    "move_motion_features = [i for i in range(inference_length+max_keyframe) if i != 1]\n",
    "\n",
    "\n",
    "# Depth_pred_idx: which depth idx to predict on the prepared motion feautes \n",
    "# Aling depth idx: On which predicted idx to aling \n",
    "\n",
    "\n",
    "distance_to_batch = [keyframe_list[idx] + (inference_length-len(keyframe_list)) for idx in range(len(keyframe_list))]\n",
    "\n",
    "static_keyframes = []\n",
    "for idx in range(len(keyframe_list)):\n",
    "    if inference_length > keyframe_list[idx]:\n",
    "        static_keyframes.append(inference_length - keyframe_list[idx])\n",
    "    elif inference_length <= keyframe_list[idx]:\n",
    "        static_keyframes.append(idx+1)\n",
    "assert not len(static_keyframes) != len(set(static_keyframes)), f'Setup leads to duplicates in the keyframes: {static_keyframes}'\n",
    "\n",
    "\n",
    "\n",
    "Depth_pred_idx = []\n",
    "Align_idx = []\n",
    "use_feature_idx = []\n",
    "for frame_idx in range(inference_length-1, inference_length+max_keyframe, 1):\n",
    "\n",
    "    tmp_batch_idx = [idx for idx in range(frame_idx-(inference_length-1), frame_idx, 1)]\n",
    "    tmp_batch_idx[0] = 0 # We always use first frame as reference\n",
    "    \n",
    "    for idx, static_keyframe in enumerate(static_keyframes):\n",
    "        if static_keyframe in tmp_batch_idx:\n",
    "            continue\n",
    "        else:\n",
    "            if frame_idx-distance_to_batch[idx] <= static_keyframe:\n",
    "                tmp_batch_idx[idx+1] = static_keyframe\n",
    "            else:\n",
    "                tmp_batch_idx[idx+1] = static_keyframe + (frame_idx - distance_to_batch[idx] - static_keyframe)\n",
    "    \n",
    "    use_feature_idx.append(tmp_batch_idx)\n",
    "\n",
    "# up to inference_length+max_keyframe use_featuer_idx[:len(keyframe_list)+1] == Align_idx == Depth_pred_idx, da alles was bisher predicted wurde drin auch noch gespeichert ist. \n",
    "# Danach schiebt sich Align_idx und Depth_pred_idx immer um die differenz von frame_idx - inference_length+max_keyframe weiter. \n",
    "\n",
    "print(use_feature_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_batch_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13, 6]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_to_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'first_frame_depth_idx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mfirst_frame_depth_idx\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'first_frame_depth_idx' is not defined"
     ]
    }
   ],
   "source": [
    "first_frame_depth_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "static_keyframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "video-depth-any",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
